\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage[binary-units]{siunitx}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\newcommand{\bigdata}{Big Data\xspace}
\newcommand{\bigbrain}{BigBrain\xspace}

\newcommand{\valerie}[1]{\color{blue}\textbf{Note from Valerie}:
      #1 \color{black}}
\newcommand{\tristan}[1]{\color{orange}\textbf{Note from Tristan}:
      #1 \color{black}}

\begin{document}

\title{Performance benefits of Intel Optane DC storage for the parallel processing of large neuroimaging data}

\author{\IEEEauthorblockN{Val\'erie Hayot-Sasson$^1$, Shawn T Brown$^2$ and 
    Tristan Glatard$^1$
  }\\
  \IEEEauthorblockA{
    $^1$Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada\\
    $^2$Montreal Neurological Institute, McGill University, Montreal, Canada
  }
}
\maketitle

\begin{abstract}
    Open-access neuroimaging datasets have reached up to $\SI{}{\peta\byte}$ of data, and
    continue to grow. The ability to leverage the entirety of these datasets are
    limited to a restricted number of labs with both the capacity and infrastructure
    to process the data. Whereas BigData engines have significantly reduced
    application performance penalties with respect to data movement, their applied
    strategies are not necessarily practical within neuroimaging workflows where
    intermediary results may need to be materialized to shared storage for post-processing
    analysis. In this paper we evaluate the performance advantage brought by Intel
    Optane DC persistent memory storage for the processing of large neuroimaging 
    datasets using the two available configurations modes: Memory Mode and App Direct Mode.
    We employ a synthetic algorithm on the $\SI{76}{\gibi\byte}$ \bigbrain
    and a real neuroimaging application on the Consortium for Reliability and Reproducibility (CoRR)
    dataset using 25 and 96 parallel
    processes in both cases. Our results show that performance of all available storage
    devices (e.g. tmpfs, local disk and Isilon) is somewhat deteriorated by memory mode
    when compared to App Direct Mode. In contrast, Optane in App Direct Mode was
    found to be slower than local disk, when the load on the bus was high due to sheer
    amount of concurrent processes and short task duration. It is believed that
    Optane would likely be useful in the accessing of typically larger-than-memory
    data in a limited-parallelism manner, such as with visualization applications.
    Further experiments with larger datasets remain to be performed.

\end{abstract}

\section{Introduction}
Neuroimaging open-data initiatives have led to extensively large repositories of
publicly available data. Such initiatives include the \bigbrain~\cite{BigBrain}, 
a one-of-a-kind $\SI{603}{\gibi\byte}$
histological image of a 65 year-old healthy human brain
at 20$\mu$m resolution; the UK Biobank~\cite{ukbiobank}, a repository expected to
contain approximately $\SI{0.2}{\peta\byte}$ of data (including various magnetic
resonance (MR) imaging modalities) from 500,000 individuals living in the UK;
the Human Connectome Project~\cite{HCP}, a repository containing MR scans from
1,200 healthy adults, which is expected to exceed $\SI{2}{\peta\byte}$ in size; 
and the Consortium for Reliability and Reproducibility (CoRR)~\cite{corr}, an
initiative which aggregates MR data from various centres around the world, 
of which 32 are currently available and make up about $\SI{937}{\giga\byte}$ of
data in total.

Due to storage limitations, only subsets of such neuroimaging repositories 
can be processed in a typical research laboratory. Moreover, as these datasets 
are extremely large and are only increasing in size, they are typically stored 
in higher-capacity, slower storage devices, such as hard disk drives, or external
parallel file systems, such as Lustre. In such conditions, large-scale 
studies in neuroimaging remain limited to labs with access to adequate 
infrastructure.

Furthermore, intermediary data is often required for post-processing analysis. This limits
any performance benefits that can arise from volatile in-memory computing as intermediary
results need to be materialized onto persistent storage. As neuroimaging datasets continue
to increase in size, the movement of data will significantly increase the processing time.

To mitigate the effects of data writes, kernel-based
strategies,such as the writeback cache, have been developed. The writeback cache
allows processes to use the memory as a cache for writing. The cache size,
however,
is configurable, but nevertheless limited. When writes approach the cache's capacity,
processes performing writes start to be throttled. After capacity is reached,
processes can no longer use the cache until all written data on cache is flushed
to the appropriate storage device.

Whereas Operating Systems and popular Big Data engines, such as MapReduce~\cite{mapreduce} and Apache Spark~\cite{spark}, have
incorporated software solutions to limit data transfers (e.g. writeback cache, in-memory computing,
data locality, and lazy evaluation), hardware has also adapted to the growing datasets.
One such improvement is the concept of placing flash storage directly on the Dual
In-line Memory Module (DIMM), thereby reducing the latency of accessing data on 
flash storage devices. While the latency of these devices is improved, the bandwidth
remains the same. However, as noted by \cite{nvdimms}, a severe performance degradation
can be experienced by having memory traffic and I/O placed on the same bus.


Intel Optane DC Persistent Memory storage~\cite{optanebrief} is a recently introduced high-performance
storage technology that also sits on the DIMMs to reduce latency to the device.
Unlike DRAM DIMMs, it can accommodate twice the volume ($\SI{512}{\giga\byte}$) of
storage enabling it to be a more cost effective alternative to DRAM storage.
There are two principle configuration modes, namely Memory Mode and App Direct Mode,
that enable the storage to either be accessed as an extension of available memory 
or as a non-volatile memory storage device.


In this paper, we aim to:
\begin{itemize}
        \item Quantify the added value of Intel Optane DC Persistent Memory on 
            processing large neuroimaging data using representative pipelines; and
        \item Determine when a given Intel Optane storage configuration (Memory 
            and App Direct Mode) is preferable.
\end{itemize}

\section{Materials and Methods}
The application pipelines, benchmarks, performance data, and analysis scripts used 
to implement the methods described hereafter are all available at 
\url{https://github.com/valhayot/paper-memory-storage} for 
further inspection and reproducibility.

\subsection{Infrastructure}

The server used in our experiments contain $\SI{3}{\tera\byte}$ of Intel Optane Persistent memory, 768GB of DRAM,
Isilon, local disk(SSD).
CPU: Cascade lake
OS: Red Hat 7.1
writeback enabled for local disk (unsure about Isilon)
\valerie{include storage bandwidth table here}

\subsection{Storage configuration}

\subsubsection{Memory Mode}

Memory Mode allows Intel Optane DC persistent memory to extend the system's available
memory. In this mode, Optane uses DRAM as a cache and Optane can be accessed as
volatile addressable main memory. By extending main memory, Memory Mode enables the
fast access of large volumes of data.

As Memory Mode would affect the processing time regardless of the device
selected, we evaluated Memory Mode on tmpfs, local disk and Isilon. tmpfs
consisted of $\SI{298}{\giga\byte}$ and was located in \texttt{/run/user/61218}.
Local disk comprised of a $\SI{240}{\giga\byte}$ Micron 5100 PRO SSD, of which
$\SI{150}{\giga\byte}$ were mounted on \texttt{/home}. The $\SI{628}{\tera\byte}$
Isilon was mounted as NFS4 to \texttt{/home/users}

Optane was not evaluated separately as it is indistinct from DRAM memory in
Memory Mode
\subsubsection{App Direct Mode}

App Direct Mode enables Optane to be accessed as a high-performance storage device.
Unlike Memory Mode, the OS is able to differentiate between main memory and Optane
storage, treating them as two distinct entities. As Optane is located on
the DIMMs, it exhibits lower latencies than traditional SSDs, while being able
to be accessed like an SSD. Optane does not use the DRAM as cache in App Direct Mode.

The same storage devices were used to evaluate App Direct Mode, with the inclusion
of Intel Optane DC Persistent Memory, of which $\SI{1.5}{\tera\byte}$ of storage
were mounted on \texttt{/pvme-1}.


\subsection{Performance model}

For the purposes of these experiments, we use the inequality introduced in
\cite{paperinmem} to characterize data-intensive applications. That is,
\begin{equation}                                                                 
\frac{D}{C} \leq \frac{\delta}{\gamma} \label{eq:page-cache-inequality}                                                                                                                                  
\end{equation} 
where:
\begin{itemize}
    \item $D$ is the total amount of data written by the application
    \item $C$ is the total execution time of the application
    \item $\delta$ is the disk bandwidth
    \item $\gamma$ is the max number of concurrent processes on a node
\end{itemize}

Using this inequality, we anticipate that whenever the application is within the limits,
the storage selection will make little difference, as storage devices with writeback enabled 
will be able to leverage the memory cache. It is expected that Optane in App Direct Mode
will be most useful when this inequality is not satisfied as storage devices will
not be able to use the writeback cache. As Memory Mode will inevitably
increase the amount of data that can be cached by the storage, it is expected that the overall
performance of all storage devices improve, even when the inequality is not satisfied.

\subsection{Applications}
\subsubsection{ \bigbrain Incrementation}

\begin{algorithm}\caption{Incrementation}\label{alg:incrementation}
    \begin{algorithmic}[1]
    \Input
        \Desc{$x$}{a sleep delay in seconds}
        \Desc{$n$}{a number of iterations}
        \Desc{$C$}{a set of image chunks}
        \Desc{$fs$}{filesystem to write to (tmpfs, Optane, local disk, Isilon)}
    \EndInput
    \ForEach{$chunk \in C$}
        \State read $chunk$ from $fs$
        \For{$i \in [1, n]$}
            \State $chunk\gets chunk+1$
            \State sleep $x$
        \EndFor
        \State save $chunk$ to $fs$
    \EndFor
    \end{algorithmic}
\end{algorithm}  

Due to the size and uniqueness of the \bigbrain, standardized processing pipelines
have yet to be developed for it. In order to quantify the effects of data location
on such a dataset, we have implemented a naive synthetic application that takes 
the image, split into blocks, and increments all the voxels within each block, in parallel (Algorithm~\ref{alg:incrementation}).
This application
enables us to read and write to different storage devices and ensure that written
data could not have been previously cache
in-memory, by ensuring that read and written data are not the same. This application
was parallelized us in two different ways: using both GNU Parallel and Apache Spark 2.4.3 (PySpark). 
All task code was implemented in Python 3.6 \tristan{In a subsequent 
version you could give more details about implementations, including containerization, etc}. While the GNU Parallel application
is currently limited to 1 incrementation step, the Apache Spark implementation allows us to increase
the number of incrementation steps by a variable amount, thereby increasing the amount of intermediate
data generated.

Time is measured within the application using Python's \texttt{time} module 
called before and after any reads, increments or writes.

We have executed this pipeline on both the $\SI{76}{\gibi\byte}$ 40~$\mu$m 
\bigbrain split into 125 $\SI{617}{\mega\byte}$ blocks and the $\SI{603}{\gibi\byte}$
\bigbrain at 20~$\mu$m split into 1000 $\SI{617}{\mebi\byte}$ blocks.

For the 40~$\mu$m \bigbrain, we have executed the application using GNU Parallel for parallelization
using 25 and 96 processes on the 125 40$\mu$m \bigbrain blocks. Data was read and written to either
tmpfs, Optane (App Direct only), local disk and Isilon. In all configurations (25 and 96 processes)
only 1 iteration was performed. At 25 processes, a sleep delay of 120 seconds was added at the
incrementation step, which would emulate tasks of longer duration and ensure that we satisfy Equation~\ref{eq:page-cache-inequality} for all devices.
At 96 processes with 0 sleep delay, we estimate being above the inequality for local disk and Isilon.
All experiments were repeated 5x in each environment (App Direct and Memory Mode).

For the 20~$\mu$m \bigbrain, we used the same application, however in this case, it was executed
using both Apache Spark and GNU Parallel. The configuration was also generally the same, having 
experiments using both 25 and 96 cpus. As the previous experiments would have already shown how
page cache availability for the dirty writes can compare with Optane, we did not include
a task duration delay. Rather, we just measure the performance differences between the various
storage devices, in addition to comparing how each device scales by modifying the cluster size.

Since storage was limited on tmpfs and local disk and was not large enough to process the entire
20~$\mu$m \bigbrain, we had to emulate the reading and writing to these devices. We achieved this
by copying only 70 of the 1000 blocks onto the storage devices, but used 1000 tasks to read, increment and
write these 70 blocks.

\subsubsection{BIDS App Example}

The BIDS App Example is a template example for creating a Brain Imaging Data Structure (BIDS)
compliant application. It runs a standard neuroimaging brain extraction application (FSL BET) 
on all the anatomical (T1W) images of a dataset containing numerous subjects. This step is 
referred to as Participant analysis, within the BIDS App Example. An optional
step of the BIDS App example, referred to as Group analysis, computes the average brain
mask size of the entire dataset.

For our experiments, we used the entire CoRR dataset, made available on DataLad and
applied Participant analysis on it. As in the BigBrain Incrementation, the experiment parallelized 
using GNU Parallel with 25 and 96 processes. The conditions were executed in both App Direct and Memory Mode
using Isilon, local disk, Optane and tmpfs as storage. As participant analysis operates on only a subset of the dataset,
the inequality is satified for all conditions. Each experiment was repeated 3x.

Time in this application was obtained using Linux's \texttt{time(1)} application.
\texttt{Real + Sys} was used to measure CPU time,
whereas \texttt{User - (Real + Sys)} was used to measure
I/O time.
\section{Results}

\subsection{40~$\mu$m \bigbrain incrementation}
\subsubsection{25 Processes}
\begin{figure}
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/makespan-real-25cpus-120delay-1it.pdf}
    \captionsetup{width=\columnwidth}
    \caption{Makespan of \bigbrain Incrementation using 25 processes with a sleep delay of 120s on 125 blocks on the 40~$\mu$m image given different storage configurations. 5 repetitions were performed.}\label{fig:makespan-25cpus}
\end{figure}
\begin{figure}
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/stacked-real-25cpus-120delay-1it.pdf}
    \captionsetup{width=\columnwidth}
    \caption{Total read/increment/write breakdown of \bigbrain Incrementation using 25 processes with a sleep delay of 120s on 125 blocks on the 40~$\mu$m image given different storage configurations. 5 repetitions were performed.}\label{fig:stacked-25cpus}
\end{figure}
\begin{figure*}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/gantt-1567709282-tmpfsAD_1it_25cpus_120delay_withpc-2.pdf}
    \caption{tmpfs}
\end{subfigure}
\begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/gantt-1567705458-optaneAD_1it_25cpus_120delay_withpc-2.pdf}
    \caption{Optane}
\end{subfigure}
\begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/gantt-1567704324-localAD_1it_25cpus_120delay_withpc-2.pdf}
    \caption{local disk}
\end{subfigure}
\begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/gantt-1567707833-isilonAD_1it_25cpus_120delay_withpc-2.pdf}
    \caption{Isilon}\label{fig:gantt25isilon}
\end{subfigure}
\caption{Gantt charts for each storage device (App Direct Mode) processing 125 blocks of the 40$\mu$m BigBrain using 25 processes and a sleep delay of 120s}\label{fig:gantt25}
\end{figure*}

As shown in Figure~\ref{fig:makespan-25cpus}, despite being under the inequality, the
performance still varies according to selected storage device. As expected, Optane performs
slightly better than local disk and significantly better than Isilon. Tmpfs outperforms all
devices, with only slightly better performance than Optane and local disk, in both App Direct Mode and Memory Mode.
There is no apparent difference between the two modes on the given devices.

When observing the total task breakdown (Figure~\ref{fig:stacked-25cpus}), it is
apparent that the differences between the makespans arise from difference in incrementation
time between all devices except Isilon. In addition, while tmpfs and local disk have comparable write times, Optane
has a more significant write time difference. Whereas, Isilon has more significant read and
write time compared to all other devices, its incrementation time is comparable to that
of local disk. Further inspection of each storage device's Gantt chart (see Figure~\ref{fig:gantt25})
shows that increased duration of tasks stagger the execution of each individual tasks. This, in turn,
results in better performance towards the end of the execution, where tasks are significantly more staggered.
Only tmpfs appears to be able to execute each task efficiently with little staggering.
\subsubsection{96 Processes}
\begin{figure}
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/makespan-real-96cpus-0delay-1it.pdf}
    \captionsetup{width=\columnwidth}
    \caption{Makespan of \bigbrain Incrementation using 96 processes on 125 blocks on the 40~$\mu$m image given different storage configurations. 5 repetitions were performed.}\label{fig:makespan-96cpus}
\end{figure}
\begin{figure}
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/stacked-real-96cpus-0delay-1it.pdf}
    \captionsetup{width=\columnwidth}
    \caption{total read/increment/write breakdown of \bigbrain incrementation using 96 processes on 125 blocks on the 40~$\mu$m image given different storage configurations. 5 repetitions were performed.}\label{fig:stacked-96cpus}
\end{figure}

\begin{figure*}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/gantt-1568150082-local_1it_96cpus_0delay_npc-3.pdf}
    \caption{Memory Mode}
\end{subfigure}
\begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/gantt-1567714252-localAD_1it_96cpus_0delay_npc-3.pdf}
    \caption{App Direct Mode}
\end{subfigure}
\caption{Gantt charts for local disk processing 125 blocks of the 40$\mu$m BigBrain using 96 processes with no delay}\label{fig:gantt96}
\end{figure*}

At 96 processes, it was expected that the application would not satisfy the inequality.
In Figure~\ref{fig:makespan-96cpus} we see that there is now a greater disparity between the
makespans of experiments executed in Memory and App Direct Mode, given the same storage. Local disk
in App Direct Mode is significantly faster than its Memory Mode counterpart. Local disk in App Direct
mode is even faster than Optane, by a factor of 1.7. Tmpfs was also found to perform better in App Direct Mode, although
the performance difference is not significant. Only Isilon's performance did not vary between the
two execution modes.

As with 25 processes, the main cause of performance variability appears to be the incrementation
time (Figure~\ref{fig:stacked-96cpus}). Local disk running in App Direct Mode has an average
incrementation time that is nearly 15~x faster than in Memory Mode. The incrementation time
for local disk is, on average, 6~x faster than on Optane.

Isilon in App Direct generally performed the same as in memory mode, however, the write
was almost 4000~seconds slower, on average, in App Direct Mode.

A further look at the Gantt charts between Memory Mode and App Direct Mode (Figure~\ref{fig:gantt96})
shows that the sole difference between the two modes is the duration of the incrementation step.
While both modes have an increased incrementation time, the incrementation duration in
Memory Mode is much more important than that of App Direct Mode.

\subsection{20~$\mu$ \bigbrain incrementation}
\subsubsection{25 Processes}

Although tmpfs offers the best performance in terms of makespan duration, Optane
is a close contender. While tmpfs is approximately 5x faster than Optane, Optane can be
as much as 12x faster than on Isilon (Figure~\ref{fig:20mksp25}). By using the results of 
our emulation, we can speculate that Optane will also be significantly faster than 
local disk in a real scenario. The real Optane execution is almost 3x faster than
the fastest local disk emulation configuration (i.e. Memory mode - Spark). As the
emulation results have been found to be faster than the real results, potentially as a side-effect
of increased accessed to cached data, it is expected
that the true makespan of the application running on local disk would be significantly
greater.

The read/increment/write breakdown (Figure~\ref{fig:20stackedp25}) shows that tmpfs
spends the least amount of time executing all the tasks, however, it spends more
time than Optane incrementing the data. Otherwise, tmpfs is nearly 470x faster with
reading and nearly 11x faster at writing than Optane. Despite not reaching memory
speeds, Optane is significantly faster than other persistent storage devices.
For instance, it can be almost up to 10x faster in reads than Isilon and almost 
9x faster in writes. It is important, however, to note that total read times in App
Direct mode for the Spark execution on Isilon are almost twice as fast as their 
equivalent Optane counterpart. On the other hand, this decrease read time is offset
by the write time which are 9x slower.

Emulated breakdown results also agree that the bulk of the differences in makespans
can be attributed to write time, with local disk writes being notably slower than
Optane. In fact, local disk write speeds nearly match that of Isilon. Isilon results
in a longer makespan due to increase read and incrementation time.
\begin{figure*}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/makespan-real-20bb_25cpus.pdf}
    \caption{real}
\end{subfigure}
\begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/makespan-em-20bb_25cpus.pdf}
    \caption{emulated}
\end{subfigure}
\caption{Makespan of the incrementation algorithm processing the 20~$\mu$m BigBrain using
25 processes}\label{fig:20mksp25}
\end{figure*}

\begin{figure*}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/stacked-real-20bb_25cpus.pdf}
    \caption{real}
\end{subfigure}
\begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/stacked-emulated-20bb_25cpus.pdf}
    \caption{emulated}
\end{subfigure}
\caption{Total read/increment/write breakdown of the  incrementation algorithm processing the 20~$\mu$m BigBrain using
25 processes}\label{fig:20stackedp25}
\end{figure*}

\subsubsection{96 Processes}

With nearly a four-fold increase in parallelization, the makespan generally did
not improve other but a few 100s on Isilon. Overall, the general trend is similar
than with 25 cpus. As can be seen in Figure~\ref{fig:20mksp96}, Optane in Memory
mode is the fastest storage device. Furthermore, Optane in App Direct mode 
remains significantly faster than the other storage devices (e.g. Isilon).
However, emulated disk results appear to imply that the speedup difference
between Optane in App Direct mode and local disk will be smaller than at 25 cpus.

The vast majority of processing time appears to be spent on writing to the 
respective storage device with the exception of Optane in Memory Mode, which 
spends most of its time incrementing (Figure~\ref{fig:20stackedp96}). App Direct
mode executions appear to have reduced incrementation time coupled with increased
write times when compared to their memory mode equivalents. Furthermore, read
times appear to be increased for Isilon memory mode executions. These events are
not reflected in the emulated executions.

Emulated breakdowns demonstrate that local disk executions will remain slower 
than Optane in both App Direct and Memory modes. Results show that both 
incrementation and write times will be generally be longer on local disk than
on Optane.


\begin{figure*}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/makespan-real-20bb_96cpus.pdf}
    \caption{real}
\end{subfigure}
\begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/makespan-em-20bb_96cpus.pdf}
    \caption{emulated}
\end{subfigure}
\caption{Makespan of the incrementation algorithm processing the 20~$\mu$m BigBrain using
96 processes}\label{fig:20mksp96}
\end{figure*}

\begin{figure*}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/stacked-real-20bb_96cpus.pdf}
    \caption{real}
\end{subfigure}
\begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bigbrain-incrementation/figures/stacked-emulated-20bb_96cpus.pdf}
    \caption{emulated}
\end{subfigure}
\caption{Total read/increment/write breakdown of the  incrementation algorithm processing the 20~$\mu$m BigBrain using 96 processes}\label{fig:20stackedp96}
\end{figure*}
\subsection{BIDS App Example}
\begin{figure*}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bids-app-example/25cores.pdf}
    \caption{25 processes}\label{fig:bm25}
\end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bids-app-example/96cores.pdf}
    \caption{96 processes}\label{fig:bm96}
\end{subfigure}
\caption{Makespan of BIDS App Example 25 and 96 processes on all storage devices. 3 repetitions were performed}
\end{figure*}

\begin{figure*}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bids-app-example/25cores-sum.pdf}
    \caption{25 processes}\label{fig:bb25}
\end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
    \includegraphics[width=\columnwidth]{./experiments/bids-app-example/96cores-sum.pdf}
    \caption{96 processes}\label{fig:bb96}
\end{subfigure}
\caption{I/O and CPU breakdown of BIDS App Example 25 and 96 processes on all storage devices. 3 repetitions were performed}\label{fig:bbd}
\end{figure*}
Unlike the BigBrain incrementation at 25 Processes, the makespan of the BIDS App Example
executing on Optane is longer than local disk (Figure~\ref{fig:bm25}). However, similarly
to the BigBrain Incrementation, tmpfs has a slight lead over local disk and Isilon performs the
worst. The same pattern can be found with 96 process (Figure~\ref{fig:bm96}). When comparing
the executions of 25 and 96 processes, the performance improved by a similar factor on all
storage devices.

The CPU and I/O breakdowns (Figure~\ref{fig:bbd}) show that there is a significant increase
in the time spent on I/O at 96 processes when compared to 25 processes. Furthermore, there appears
to also be an increase in the amount of time spent on CPU processing. In both cases,
tmpfs spends the least amount of time on I/O and CPU, followed by local disk, Optane, and finally,
Isilon.
\section{Discussion}
\subsection{Memory vs App Direct Mode}

Memory mode was found to result in increased computation time. When processing 
the 20~$\mu$m \bigbrain, Memory mode results indicate that more time was spent on
the incrementation tasks than its App Direct counterparts. Unlike App Direct mode,
which relies exclusively on DRAM for working memory, Memory mode relies on Optane
with a DRAM cache. Optane, while faster than other persistent storage devices, 
still lacks the bandwidth of DRAM. Should anonymous memory need to access Optane
for computation, it is not surprising that it would be slower than a DRAM-only
access. However, whereas Memory mode experienced increased computation during the
processing of the 20~$\mu$m \bigbrain, this was not always found to be the case
with the processing of the 40~$\mu$m \bigbrain. In fact, incrementation time was
largely similar between the two modes, with the notable exception of Optane in
Memory mode expending significantly less computation time than in App Direct when
processing the 40~$\mu$m \bigbrain with both 25 and 96 processes. Optane in 
Memory mode executing on local disk using 96 cpus, however, required significant
computation time as compared to App Direct. 
- Conclusion: memory mode should only be used when there's a requirement for additional memory
-App Direct mode results in increased I/O time for Optane
- conclusion: Optane might benefit from the DRAM cache



While Memory Mode provides a cost-effective solution to increasing the total memory
size, it is slower than App Direct Mode for embarrassingly parallel applications such
as \bigbrain incrementation. This is likely due to the Page Cache being composed of a
combination of both DRAM and Optane in Memory Mode. Since the data produced by the application should be
able to fit within DRAM, writing to a slower bandwidth device such as Optane inevitably
deteriorates performance. As App Direct separates usage of DRAM and Optane, devices only utilize DRAM in this mode.
The only device that performed better in Memory Mode was 
Isilon, however the difference is minimal.
%This may be explained by a larger Page Cache in Memory Mode, enabling Isilon to
%store more data in memory prior to initiating synchronous writes to Isilon.

I/O-less task duration also seem to be affected by the execution mode, with those tasks
also performing significantly better in App Direct. A potential explanation for this may be 
that all memory accesses to Optane are slower due to the device's bandwidth, meaning that
tasks that do not perform I/O, but access data loaded into memory are slower in Memory Mode. Another explanation might be that the kernel is throttling the processes
as they are producing significant amounts of output, thereby increasing the
incrementation time.
This can be seen in Figure~\ref{fig:gantt96} where for local disk, incrementation in App Direct
mode is significantly shorter. Only when the memory bus is overwhelmed does incrementation take
longer for App Direct, however, it is nothing compared to the average duration of the incrementation
tasks in Memory Mode. 

Once again, Isilon is the only device which does not experience an increase in duration for
I/O-less tasks. In fact, the incrementation duration is less than that of local disk in Memory Mode
and Optane in App Direct Mode. It may be that since even the reads are staggered
(Figure~\ref{fig:gantt25isilon}), the memory bus is less strained by all the parallel processes,
therefore incurring less of an overall delay for all memory accesses.


\subsection{Effect of Page Cache}

As long as all the written data does not exceed the dirty ratio, there is little 
benefit brought by the use of Optane. This is the case with both
our applications writing less than $\SI{1}{\gibi\byte}$ to storage on a system with 
$\SI{768}{\gibi\byte}$ of DRAM and a dirty ratio of 20. However, it is expected 
that as the memory footprints of applications increase to the point where the kernel
would be forced to perform synchronous writes to disk if it were to rely on DRAM alone,
the benefit of using Optane in Memory Mode will become more apparent. 

However, depending on the neuroimaging application, the amount of data that could be
loaded in parallel would be limited by the amount of parallelism possible on the system.
For instance, typical fMRI tasks process many small images (100s of MB in size) in parallel.
While the datasets themselves can reach PB in size, the amount of data that can be processed in
parallel can be $\SI{10}{\giga\byte}$ if each take processes $\SI{100}{\mega\byte}$ and the 
maximum amount of parallelism is 100. 

On the other hand, Memory Mode can be very useful for applications operating on
ultra-high resolution images, like the \bigbrain. For instance, visualization applications
many need to load significant amounts of data in memory. Given the size of these datasets, 
extending the amount of available memory using Optane could be quite beneficial and allow
for faster visualization of larger datasets.


\subsection{Added value of persistent memory}

The added value of Optane as a persistent storage device depends on the amount of parallelism
of the application. For instance, Optane performs better than local disk at 25 processes,
but worse than local disk at 96 processes. This can be explained by the fact that it performs
I/O on the memory bus. While this may reduce overall latency to Optane, the bandwidth of 
writing to Optane is still significantly less than DRAM. Despite Optane having a similar bandwidth
to local disk, local disk uses DRAM as cache whereas Optane does not. This fact is 
particularly evident with the amount of data that is written in parallel at 96 processes. With 25 processes
and at least a 2 min
delay between each write, the load on the memory bus is low enough to be efficiently handled.

fMRI application will generally attempt to leverage as much parallelism as possible. As this
is the case, it is not believed that App Direct Mode would bring a significant performance advantage
unless the server's CPUs are limited, in which case there is less concurrent accesses on the device.
The same would apply to ultra-high resolution application unless they are sequential, as may be the case with
visualization.
\section{Conclusion}

The benefits of Optane for neuroimaging applications are limited to large memory
accesses with limited parallelism, as would be the case for visualization. This is
the case for both Memory Mode and App Direct Mode.

While Memory Mode leads to deterioration in performance of non-Optane storage devices,
they act as expected in App Direct Mode. It is currently unknown by how much performance
of a non-Optane storage device would be affected by a large memory access.

To be able to fully determine the benefits of Optane for neuroimaging application,
it is necessary to process larger datasets. We plan to execute incrementation pipeline on the 
20~$\mu$m BigBrain using Apache Spark. 
\section{Acknowledgement}
\bibliographystyle{IEEEtran} 
\bibliography{biblio}

\end{document}
